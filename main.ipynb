{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
    "    data = load_dataset(\"openai/gsm8k\", \"main\")[split]  # type: ignore\n",
    "    data = data.map(\n",
    "        lambda x: {  # type: ignore\n",
    "            \"prompt\": \"test\",\n",
    "            \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "        }\n",
    "    )  # type: ignore\n",
    "    return data  # type: ignore\n",
    "\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(completions, **kwargs):\n",
    "    return [1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(output_dir=\"some_path\", logging_steps=1)\n",
    "trainer = GRPOTrainer(\n",
    "    model=\"gpt2\",\n",
    "    reward_funcs=correctness_reward_func,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Alignment Handbook utils\n",
    "import os\n",
    "import re\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "    task: Literal[\"sft\", \"generation\", \"rm\", \"dpo\"] = \"sft\",\n",
    "    assistant_prefix=\"<|assistant|>\\n\",\n",
    "):\n",
    "    def _strip_prefix(s, pattern):\n",
    "        # Use re.escape to escape any special characters in the pattern\n",
    "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
    "\n",
    "    if task in [\"sft\", \"generation\"]:\n",
    "        messages = example[\"messages\"]\n",
    "        # We add an empty system message if there is none\n",
    "        if messages[0][\"role\"] != \"system\":\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "        example[\"text\"] = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True if task == \"generation\" else False,\n",
    "        )\n",
    "    elif task == \"rm\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            chosen_messages = example[\"chosen\"]\n",
    "            rejected_messages = example[\"rejected\"]\n",
    "            # We add an empty system message if there is none\n",
    "            if chosen_messages[0][\"role\"] != \"system\":\n",
    "                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            if rejected_messages[0][\"role\"] != \"system\":\n",
    "                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(\n",
    "                chosen_messages, tokenize=False\n",
    "            )\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(\n",
    "                rejected_messages, tokenize=False\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    elif task == \"dpo\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
    "            prompt_messages = [\n",
    "                [msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]\n",
    "            ]\n",
    "            # Insert system message\n",
    "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            else:\n",
    "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
    "            # TODO: handle case where chosen/rejected also have system messages\n",
    "            chosen_messages = example[\"chosen\"][1:]\n",
    "            rejected_messages = example[\"rejected\"][1:]\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(\n",
    "                chosen_messages, tokenize=False\n",
    "            )\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(\n",
    "                rejected_messages, tokenize=False\n",
    "            )\n",
    "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
    "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            example[\"text_chosen\"] = _strip_prefix(\n",
    "                example[\"text_chosen\"], assistant_prefix\n",
    "            )\n",
    "            example[\"text_rejected\"] = _strip_prefix(\n",
    "                example[\"text_rejected\"], assistant_prefix\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Task {task} not supported, please ensure that the provided task is one of {['sft', 'generation', 'rm', 'dpo']}\"\n",
    "        )\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "    data_config: dict,\n",
    "    splits: List[str] = [\"train\", \"test\"],\n",
    "    shuffle: bool = True,\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads one or more datasets with varying training set proportions.\n",
    "\n",
    "    Args:\n",
    "        data_config (`DataArguments` or `dict`):\n",
    "            Dataset configuration and split proportions.\n",
    "        splits (`List[str]`, *optional*, defaults to `['train', 'test']`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "\n",
    "    Returns\n",
    "        [`DatasetDict`]: The dataset dictionary containing the loaded datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data_config) is dict:\n",
    "        # Structure of the input is:\n",
    "        #     dataset_mixer = {\n",
    "        #             \"dataset1\": 0.5,\n",
    "        #             \"dataset1\": 0.3,\n",
    "        #             \"dataset1\": 0.2,\n",
    "        #         }\n",
    "        dataset_mixer = data_config\n",
    "    else:\n",
    "        raise ValueError(f\"Data config {data_config} not recognized.\")\n",
    "\n",
    "    raw_datasets = mix_datasets(dataset_mixer, splits=splits, shuffle=shuffle)\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def mix_datasets(\n",
    "    dataset_mixer: dict, splits: Optional[List[str]] = None, shuffle=True\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads and mixes datasets according to proportions specified in `dataset_mixer`.\n",
    "\n",
    "    Args:\n",
    "        dataset_mixer (`dict`):\n",
    "            Dictionary containing the dataset names and their training proportions. By default, all test proportions are 1.\n",
    "        splits (Optional[List[str]], *optional*, defaults to `None`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "    \"\"\"\n",
    "    raw_datasets = DatasetDict()\n",
    "    raw_train_datasets = []\n",
    "    raw_val_datasets = []\n",
    "    fracs = []\n",
    "    for ds, frac in dataset_mixer.items():\n",
    "        fracs.append(frac)\n",
    "        for split in splits:\n",
    "            try:\n",
    "                # Try first if dataset on a Hub repo\n",
    "                dataset = load_dataset(ds, split=split)\n",
    "            except DatasetGenerationError:\n",
    "                # If not, check local dataset\n",
    "                dataset = load_from_disk(os.path.join(ds, split))\n",
    "\n",
    "            if \"train\" in split:\n",
    "                raw_train_datasets.append(dataset)\n",
    "            elif \"test\" in split:\n",
    "                raw_val_datasets.append(dataset)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Split type {split} not recognized as one of test or train.\"\n",
    "                )\n",
    "\n",
    "    if any(frac < 0 for frac in fracs):\n",
    "        raise ValueError(\"Dataset fractions cannot be negative.\")\n",
    "\n",
    "    if len(raw_train_datasets) > 0:\n",
    "        train_subsets = []\n",
    "        for dataset, frac in zip(raw_train_datasets, fracs):\n",
    "            train_subset = dataset.select(range(int(frac * len(dataset))))\n",
    "            train_subsets.append(train_subset)\n",
    "        if shuffle:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n",
    "        else:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n",
    "    # No subsampling for test datasets to enable fair comparison across models\n",
    "    if len(raw_val_datasets) > 0:\n",
    "        if shuffle:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(\n",
    "                seed=42\n",
    "            )\n",
    "        else:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n",
    "\n",
    "    if len(raw_datasets) == 0:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_mixer} not recognized with split {split}. Check the dataset has been correctly formatted.\"\n",
    "        )\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/git/leon/trl/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-16 14:14:00,144] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = get_datasets(\n",
    "    {\"HuggingFaceH4/ultrafeedback_binarized\" : 0.005}, # 0.5% sampled\n",
    "    splits = [\"train_prefs\", \"test_prefs\"],\n",
    ")\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs = {\"tokenizer\": tokenizer, \"task\": \"dpo\"},\n",
    "    num_proc = 12,\n",
    "    remove_columns = column_names,\n",
    "    desc = \"Formatting comparisons with prompt template\",\n",
    ")\n",
    "\n",
    "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
    "for split in [\"train\", \"test\"]:\n",
    "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        report_to = \"none\",\n",
    "        output_dir=\"outputs\",\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        logging_steps=25,\n",
    "        learning_rate=5e-6,\n",
    "        bf16 = True,\n",
    "        beta = 0.1,\n",
    "        eval_steps=10,\n",
    "        #num_train_epochs=1,\n",
    "        max_steps=10,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        model_adapter_name=\"DPO\",\n",
    "        ref_adapter_name=\"reference\",\n",
    "        max_length = 256,\n",
    "        max_prompt_length = 128,\n",
    "    ),\n",
    "    train_dataset = raw_datasets[\"train\"],\n",
    "    processing_class = tokenizer,\n",
    "    eval_dataset=raw_datasets['test']\n",
    ")\n",
    "\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
